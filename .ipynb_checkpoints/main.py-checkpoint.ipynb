{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Predicting reviews of a restaurant</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The dataset contain reviews of restaurants and so we will make some machinery models that will predict if the review is positive or negative</p>.\n",
    "Columns :\n",
    "1 - review\n",
    "2 - rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV means Comma-Saparated Values, that means that the columns in the file are separated by Comma. And for the TSV is Tab-separated Values, like the first one but the separator is Tab.\n",
    "<p>Why it is better to choose a <b>TSV</b> file than a <b>CSV</b> when working with text ?.</p>\n",
    "It's common that we found in some reviews a comma like \"Food, great!\". In this example the CSV will parse this review in 2 columns. \"Food\" and \"great!\" when it's actually one column for review and it will mess all the algorithm. However, in TSV, the parser will know that is only one column because the delimiter is the TAB and a TAB can't be written in a review.\n",
    "\n",
    "For the rate's column it's equal 1 or 0, 1 means that is a positive review and 0 negative review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing the libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing the dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Restaurant_reviews.tsv\", delimiter=\"\\t\", quoting= 3)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cleaning the text</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The first step is to cleaning text. In the end, we will create a <b>Bag of Words Model</b> and this will consist of getting only the relevant words, that means that we will get rid of words like \"The\", \"on\", \"or\"... also the punctuation, and the different version of words like \"Walk\" is the same as \"Walking\".\n",
    "<p>Then we will represent the words in a vector for each reviews so it will be one column for each word. We will have a lot columns and then for each review each column will take the number of times the associate number appears in the review.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow... Loved this place.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# We will take the first review as an example\n",
    "review = dataset['Review'][0]\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>First Step : Getting rid of irrelevant caracters</h4>\n",
    "<p>caracters like numbers or punctuation will not help the algorithm to predict the result, we will let only the letters.\n",
    "We will replace the others by a space, if we just delete these, some uncomprehensible words will appears. \n",
    "Example : If we have \"Wow,There is so beautiful\". If we delete the comma, there will be a new word \"WowThere\" and of course this will make no sens.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow    Loved this place '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace the unwanted caracter by a space\n",
    "review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][0])\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow    loved this place '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting all the letters of the reviews in the lowercase\n",
    "review = review.lower()\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Second step : Let only the important words</h4>\n",
    "<p>In the reviews, some words a very important to determine if the review is positive or negative. Like in the first example \"Loved\". On the contrary, there are words who do not help the algorithm, like \"The\", these words have not impact on the review, that means if we delete these, the review nature will still be the same.</p>\n",
    "<p>The irrelevant word are in the package called <i>stopwords</i> in the NLTK library, we will download this package to work with for checking if there is such words</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this review the word that will help the algorithm to predict is \"Loved\"\n",
    "# All the irrelevant word are in a package called stopwords in the nltk library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow', 'loved', 'this', 'place']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the reviews to get every words\n",
    "review = review.split()\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Third step : Getting rid of variations of the words</h4>\n",
    "<p>To explain this, let's say we have 2 corpus :<br>\n",
    "\"I was taking a ride in the car\"<br>\n",
    "\"I was riding in the car\"<br>\n",
    "For sur, the two corpus give the same meaning. The words \"ride\" and \"riding\" here is viewed as a different words even if it is the same. We must treat these words like one. That's why we will use the Stemming. The Stemming allow us to extract the racine of the word, for our example, we have \"loved\", with the stemming the word \"loved\" will be converted to \"love\"</p>\n",
    "<p>The stemming is a powerful tools. However, some times this technique might generate some error, like the words \"University\" and \"Universe\" is viewed as \"univers\".</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow', 'love', 'place']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "# The stemmer has converted \"loved\" to \"love\"\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p>Right now, we are on a good path to reduce the future sparcity that will occur throught the creation ou bag of words model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Fourth Step : Rejoining the words to one corpus</h4>\n",
    "<p>In this step we will bring back to words to one corpus. We will put a space between each words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow love place'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = ' '.join(review)\n",
    "review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apply the cleanings step to all reviews and joining them all to one corpus</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "# Append the first example review\n",
    "corpus.append(review)\n",
    "for i in range(1, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow love place', 'crust good', 'tasti textur nasti', 'stop late may bank holiday rick steve recommend love', 'select menu great price']\n"
     ]
    }
   ],
   "source": [
    "# Print just the first five reviews\n",
    "print(corpus[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating the Bag of Words</h2>\n",
    "<p>What is the bag of words and why we do need to create it ?</p>\n",
    "<p>In the first step of processing the main goal was not only to clean the text but to create the main corpus. The bag of words consist of creating for each word one column, of course we have a lot of words so we will have a lot of columns.<br>\n",
    "For the rows in the table, it will be the reviews. Basically we will get in the table, 1000 rows for 1000 reviews and a lot of columns to each word, so each cell will correspand to one specific review and one specific word and in this cell we will have the number, and this number is going to be the number of times the word is corresponding to the column appears in the review</p>\n",
    "<p>For the first example, we will get the column for \"wow\" in the first row is equal to 1 because it appears in it, the same for \"love\". All the words remains is equal to 0 for this rows, so basically we wil\n",
    "l get a lot of 0 in the matrix. This fact is called <b>Sparcity</b><br></p>\n",
    "<p>So why do need to create this matrice ? Simply because we will predict if the review are positive or negative and we need this matrix to train the model, because for all these reviews, we have the real results of the sentiment, and the model will make some correlations between the words and the sentiment result<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The library needed for the creation of bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Here we decide to take the most 1500 frequent words\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "matrix = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>wW will have the result a matrix of 1000 rows and 1500 columns</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1500)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the dependant variable\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(matrix, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55, 42],\n",
       "       [12, 91]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the results here, we have 55 + 91 correct prediction and 42 + 12 incorrect prediction, not bad for 800 observation. Here we used naive bayes model, so the more observation the more accurate will be the prediction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.0 %\n"
     ]
    }
   ],
   "source": [
    "print(((55 + 91)/ 200)*100,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
